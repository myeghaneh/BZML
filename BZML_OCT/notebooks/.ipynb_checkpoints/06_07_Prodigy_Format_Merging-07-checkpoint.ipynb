{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex and Prodigy (Merging all enitity before correction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size= \"3\">\n",
    "Annotation of entities for sentences based on prodigy's format for different labels for further usage in Machine learning. Prodigy is an annotation tool based on spaCy. Please see here:\n",
    "    </p>\n",
    "  <a href=\"https://prodi.gy/ \"> Prodigy </a> \n",
    "<font 3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import spacy \n",
    "import re\n",
    "from prodigy.util import write_jsonl\n",
    "import prodigy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importVersion = '013'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '../data/01_df_v{0}.pickle'.format(importVersion)# Put the path of the data in your local machine here, consider the letter \"r\" before the path\n",
    "dfAstroNova = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data based on the chapters of the book \n",
    "dfAstroNova['chapter'] = dfAstroNova.chapter.replace(\"appendix b\",np.nan).astype(float)  \n",
    "dfAstroNova = dfAstroNova.rename_axis('MyIdx').sort_values(by = ['chapter', 'MyIdx'], ascending = [True, True])\n",
    "dfAstroNova.chapter.fillna('appendix b', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAstroNova.reset_index(inplace=True)\n",
    "dfAstroNova=dfAstroNova.drop(\"MyIdx\",axis=1,inplace=False)\n",
    "dfAstroNova=dfAstroNova.drop(\"html\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dfAstroNova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAstroNova.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dfAstroNova.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=[]\n",
    "for para in dfAstroNova.sentences:\n",
    "    sents +=para"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging  Based on Consistant format with Prodigy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<fig size= 3>\n",
    "We annotated the text label by label using regex, In this way we  could see influence of each label.Then, each annotation could be corrected by a annotater using prodigy interface. Morever we run a ner.batch-train to  use Prodigy to make more corrections. Now we can merge all versions of our data in a single version of corrected annotated data by annotater and prodigy.\n",
    "\n",
    "    \n",
    "<fig size>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging DATE, TIME and ASTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<fig size= 3>\n",
    "Annotations refer to:\n",
    "<ul>\n",
    "    \n",
    "<li>ASTR: Related phrases </li>\n",
    "<li>DATE: All types of date </li>\n",
    "<li>TIME: All types of time </li>\n",
    "<li>PARA: Astronomical sizes</li>\n",
    "<li>LONG: Related phrases to longituide </li>\n",
    "<li>STAR: Related phrases to stars </li>\n",
    "<li>PLAN: Related phrases to planets </li>\n",
    "<li>NAMES: important names</li>\n",
    "<fig size>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prodigy.components.db import connect\n",
    "from prodigy.models.ner import merge_spans\n",
    "\n",
    "db = connect()  # connect to the DB using the prodigy.json settings\n",
    "datasets = ['ner_date_v02','ner_time_v02','ner_para_v02','ner_astr_v03','ner_long_v10','ner_star_v02','ner_plan_v02','ner_name_v02','ner_geom_v01']\n",
    "examples = []\n",
    "for dataset in datasets:\n",
    "    examples += db.get_dataset(dataset)  # get examples from the database\n",
    "\n",
    "merged_examples = merge_spans(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prodigy import set_hashes\n",
    "\n",
    "merged_examples = [set_hashes(eg, overwrite=True) for eg in merged_examples]\n",
    "db.add_dataset('data_merged_v08')\n",
    "db.add_examples(merged_examples, datasets=['data_merged_v08'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Misaligned Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<fig size= 3>\n",
    "This step is very important, since if Pordigy faces with Misaligned tokens, we can see that before and try to adjust regex accordingly\n",
    "<fig size>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misaligned_token(examples):\n",
    "    counter=0\n",
    "    nlp = spacy.load(\"en_core_web_sm\")  \n",
    "    for example in examples:  \n",
    "        doc = nlp(example[\"text\"])\n",
    "        for span in example[\"spans\"]:\n",
    "            char_span = doc.char_span(span[\"start\"], span[\"end\"])\n",
    "            if char_span is None:  \n",
    "                counter+=1\n",
    "                print(\"{}- Misaligned tokens-->\".format(counter), example[\"text\"], span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "misaligned_token(merged_examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
