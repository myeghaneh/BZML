{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Annotation of Observation based on trained spaCy (2.0.18) model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<fig size= \"4\">\n",
    "This notebook provides a baseline annotation suggestion by using refined named entity recognition that we provided using training a deep learning method by prodigy.  </p>\n",
    "</fig size= \"4\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from spacy import displacy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"compact\": True, \"bg\": \"#09a3d5\",\n",
    "           \"color\": \"white\", \"font\": \"Source Sans Pro\",\"collapse_phrases\":False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('max_colwidth', 260)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "importVersion = '013'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/01_df_v013.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f22139534582>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'../data/01_df_v{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportVersion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Put the path of the data in your local machine here, consider the letter \"r\" before the path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdfAstroNova\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\moha\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(path, compression)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[0;32m    144\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;31m# 1) try standard libary Pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moha\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/01_df_v013.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "path= '../data/01_df_v{0}.pickle'.format(importVersion)# Put the path of the data in your local machine here, consider the letter \"r\" before the path\n",
    "dfAstroNova = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data based on the chapters of the book \n",
    "dfAstroNova['chapter'] = dfAstroNova.chapter.replace(\"appendix b\",np.nan).astype(float)  \n",
    "dfAstroNova = dfAstroNova.rename_axis('MyIdx').sort_values(by = ['chapter', 'MyIdx'], ascending = [True, True])\n",
    "dfAstroNova.chapter.fillna('appendix b', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAstroNova.reset_index(inplace=True)\n",
    "dfAstroNova=dfAstroNova.drop(\"MyIdx\",axis=1,inplace=False)\n",
    "dfAstroNova=dfAstroNova.drop(\"html\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dfAstroNova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAstroNova.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dfAstroNova.reset_index().text.str.split('.',expand=True).stack().reset_index(level=-1,drop=True)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create lists to fill with values\n",
    "l_col1 = []\n",
    "l_col2 = []\n",
    "\n",
    "# iterrate over each row and fill our lists\n",
    "for ix, row in dfAstroNova.iterrows():\n",
    "    for value in row['sentences']:\n",
    "        l_col1.append(value)\n",
    "        l_col2.append(row['chapter'])\n",
    "\n",
    "# Create new dataframe from the two lists\n",
    "df= pd.DataFrame({'sentences': l_col1 ,\n",
    "                         'chapter': l_col2 })\n",
    "df=df.rename(columns={\"sentences\":\"sents\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload customized spaCy model provioded by our training data in prodigy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= '../data/Model_V17'\n",
    "nlp=spacy.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity(x):\n",
    "    a=[]\n",
    "    b=[]\n",
    "    for ent in nlp(x).ents:\n",
    "        a +=ent.text,\n",
    "        b +=ent.label_,\n",
    "    c=list(zip(a,b))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute(x):\n",
    "    a=[]\n",
    "    b=[]\n",
    "    c=[]\n",
    "    d=[]\n",
    "    e=[]\n",
    "    for token in nlp(x):\n",
    "        a +=token.text,\n",
    "        b += token.pos_,\n",
    "        c += token.tag_,\n",
    "        d += token.dep_,\n",
    "        e += token.lemma_\n",
    "    z=list(zip(a,b,c,d,e))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def satz_analytic2(satz):\n",
    "#    merkmal = dict({\"satz\":satz.text})\n",
    "    merkmal=dict({})\n",
    "# search for main verb\n",
    "    for t in satz:\n",
    "        if t.dep_ == \"ROOT\":\n",
    "            r=t.head.text\n",
    "            merkmal.update({\"act\":r})\n",
    "# subject and object related to verb\n",
    "    for t in satz:\n",
    "        if t.dep_ == \"dobj\" and merkmal[\"act\"]==t.head.text:\n",
    "             merkmal.update({\"obj\":''.join(w.text_with_ws for w in t.subtree)})\n",
    "        if t.dep_ == \"nsubj\" and merkmal[\"act\"]==t.head.text:\n",
    "            merkmal.update({\"subject\":t.text.lower()})\n",
    "\n",
    "    return(merkmal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NerFeature(x):\n",
    "    list_en=[]\n",
    "    N=3\n",
    "    X=list(0 for i in range(0,N)) \n",
    "    if x is not None:\n",
    "        for idx in range(0,len(x)):\n",
    "            list_en.append(x[idx][1])\n",
    "            result_0=any([lst in list_en for lst in [\"DATE\"]])\n",
    "            if result_0:\n",
    "                X[0]=1\n",
    "            result_1=any([lst in list_en for lst in [\"TIME\"]])\n",
    "            if result_1:\n",
    "                X[1]=1\n",
    "            result_2=any([lst in list_en for lst in [\"LONG\"]])\n",
    "            if result_2:\n",
    "                X[2]=1 \n",
    "    else:\n",
    "        X=list(0 for i in range(0,N))\n",
    "    return X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def noun_chunk(x):\n",
    "    a=[]\n",
    "    for chunk in nlp(x).noun_chunks:\n",
    "        a.append(chunk)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['chunks'] = df['sents'].apply(lambda x: noun_chunk(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"aso\"]=df[\"sents\"].apply(lambda y: satz_analytic2(nlp(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entities'] = df['sents'].apply(lambda x: entity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fner'] = df['entities'].apply(lambda x: NerFeature(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot_observation_01(x):\n",
    "    list_en=[]\n",
    "    for idx in range(0,len(x)):\n",
    "        list_en.append(x[idx][1])\n",
    "    if any (lst in (\"LONG\",\"DATE\",\"TIME\") for lst in list_en):\n",
    "        y=1\n",
    "    else:\n",
    "        y=0 \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot_observation_02(x):\n",
    "    list_en=[]\n",
    "    for idx in range(0,len(x)):\n",
    "        list_en.append(x[idx][1])\n",
    "    result= all(lst in list_en for lst in [\"LONG\",\"DATE\",\"TIME\"])\n",
    "    if result:\n",
    "        y=1\n",
    "    else:\n",
    "        y=0\n",
    "    return y\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot_observation_03(x):\n",
    "    list_en=[]\n",
    "    for idx in range(0,len(x)):\n",
    "        list_en.append(x[idx][1])\n",
    "    result= all(lst in list_en for lst in [\"DATE\",\"TIME\"])\n",
    "    if result:\n",
    "        y=1\n",
    "    else:\n",
    "        y=0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['entities'].apply(lambda x: annot_observation_02(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.label==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_chapter(x):\n",
    "     return df.loc[df.chapter==10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(show_chapter,x=(1,70,1))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result as a JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided 3 different versions of annotation of observational sentences based on rule-based methods specifically using named entity recognition. These can be modified by a human annotator for having more precise labels. \n",
    "- A01--> If there is an entity from labels LONG, DATA, TIME in the sentences. we consider the sentence as an observational sentences \n",
    "\n",
    "\n",
    "- A02-->If there are all entities from labels LONG, DATA, TIME in the sentence. We consider the sentence as an observational sentence\n",
    "\n",
    "-  A03 -->If there are all entities from labels  DATA, TIME in the sentence. We consider the sentence as an observational sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"dfObsV01A01.json\")\n",
    "#df.to_json(\"dfObsV01A02.json\")\n",
    "#df.to_json(\"dfObsV01A03.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
